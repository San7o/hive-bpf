# hive

This evening's project is to implement a simple eBPF-based kernel
tracing tool that traces inodes accesses and logs access information
to an outside server, all of this inside Docker in a Kubernetes
environment. Moreover, the tracing points will be configured via a YAML
config file. 

As usual, I will set a maximum of 3 hours before closing this project
(excluding the time to write this document). I will not use any AI in
the process. Hopefully I've caught your attention. Let's start.

# Index
- [eBPF?](#ebpf)
- [The eBPF program](#ebpf-program)
- [The loader](#ebpf-loader)
- [The config file](#config)
- [Does It work?](#does-it-work)
- [Kubernetes Cluster](#kubernetes)
- [Further Work](#further-work)

<a name="ebpf"></a>
# eBPF?

eBPF programs are very simple and limited. They allow the use of familiar
tracing hooks, such as tracepoints, perf events, and kprobes, through
loadable programs. An eBPF program has its own [instruction set](https://www.ietf.org/archive/id/draft-thaler-bpf-isa-00.html)
which is very basic, programs are limited to having at most 512 Bytes
of stack size, loops are not allowed and functions can have up to 5
argumnets. Usually you do not write bytecode directly; instead you let a
compiler generate it for you. Traditionally, [BCC](https://github.com/iovisor/bcc)
is used to compile said programs, however, both LLVM and GCC have caught
up and now provide eBPF targets. A fundamental change to the eBPF
ecosystem was made with the introduction of the Bpf Type Format (BTF)
wich enables CO-RE (Compile Once, Run Everywhere). This is a pretty
cool system and I may write a future blog post digging deep into the
inner workings of BTF. For now, you just need to know that using BTF will
enable the program to work on any kernel version. User space
provides eBPF programs to the kernel via the `bpf(2)` syscall, which will
verify that the program is correct (enforcing the previous limitations)
and will proceed to JIT compile it. People have been using eBPF for
tracing purposes. Moreover, eBPF programs can modify the kernel
innerworkings (such as the scheduler or cache policy) and, in recent
years, people are exploring its usage, even though the technology
is quite old (the original BPF was released in 1992).

<a name="ebpf-program"></a>
# The eBPF program

Let's start writing the eBPF program. We need to import the usual
headers:
```c
#include "vmlinux.h"
#include <bpf/bpf_helpers.h>
#include <bpf/bpf_tracing.h>
#include <bpf/bpf_core_read.h>
```

`vmlinux.h` was generated by running:
```bash
bpftool btf dump file /sys/kernel/btf/vmlinux format c
```
We are using `vmlinux.h` to take advantage of BTF, we just
need to remember to use special macros when accessing any data
to ensure correct type relocations. Suppose we write our program to
access the field `bar` which has an offset of 4 in the kernel struct
`foo`. Kernel structs and functions do not guarantee consistency
with older versions, unlike user space APIs. This means that 
the type of `bar` may change, or it may be placed elsewere, making
the offset incorrect and leading to reading the wrong data. To address
this, the compiler can use BTF to relocate offsets and ensure
the code remains functional. As a side note, I have created a different `vmlinux.h`
for each supported architecture (currently only x86_64) and renamed
it based on the kernel version (e.g. `vmlinux_x86_64_6_11_4.h`)
using a small shell script. With this in mind, let's proceed with
writing a `map`.

We use maps to share data between kernel space and userspace. Our map
will contain an array of inodes to trace:
```c
#define MAP_MAX_ENTRIES 1

struct {
  __uint(type, BPF_MAP_TYPE_ARRAY);
  __type(key, __u32);
  __type(value, __u64);
  __uint(max_entries, MAP_MAX_ENTRIES);
} traced_inodes SEC(".maps"); 
```

For now, the map contains one entry, but expanding this is
trivial.
Now we begin working on the actual logic. We can attach to funcions
using kprobes. Since we want to trigger this program when an inode
is accessed, we have multiple functions to choose from. I chose `inode_permission`
since It is called before performing any operation on the inode. The
cool thing is that It also works for directories, meaning that this
function will be called on a directory if an operation was performed on any
of Its children. The kernel defines the function in `fs/namei.c`:
```c
/**
 * inode_permission - Check for access rights to a given inode
 * @idmap:	idmap of the mount the inode was found from
 * @inode:	Inode to check permission on
 * @mask:	Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)
 *
 * Check for read/write/execute permissions on an inode.  We use fs[ug]id for
 * this, letting us set arbitrary permissions for filesystem access without
 * changing the "normal" UIDs which are used for other things.
 *
 * When checking for MAY_APPEND, MAY_WRITE must also be set in @mask.
 */
int inode_permission(struct mnt_idmap *idmap,
		     struct inode *inode, int mask)
```

So our eBPF funcion will have this segnature:
```
SEC("kprobe/inode_permission")
int kprobe_inode_permission(struct pt_regs *ctx)
```

The `SEC` macro places the funcion in the specified section. This
is a common convention introduced by bpftool, which is the "standard
library" for writing eBPF programs. The function takes the register
as argument, as usual. We can then parse the argument that we want
by looking at the original funcion's signature and using the ugual
calling convention for registers. We are using `BPF_CORE_READ` to
resolve BTF relocation:
```c
struct mnt_idmap *idmap = (struct nmt_idmap*) BPF_CORE_READ(ctx, di);
struct inode *inode = (struct inode*) BPF_CORE_READ(ctx, si);
int mask = (int) BPF_CORE_READ(ctx, dx);
```

Finally we compare the value of the map with the inode argument, If they
match, a message will be printed in `/sys/kernel/debug/tracing/trace_pipe`.
We are using `BPF_CORE_READ` again.
```c
__u32 key = 0;
__u64 initval = 1, *valp;
long unsigned int ino = BPF_CORE_READ(inode, i_ino);
valp = bpf_map_lookup_elem(&traced_inodes, &key);
if (valp && *valp == ino)
{
  int pid = bpf_get_current_pid_tgid() >> 32;
  bpf_printk("{pid = %d, inode = %ld\n}", pid, ino);
}
```

Do not forget the license:
```c
char LICENSE[] SEC("license") = "Dual MIT/GPL";
```

To use many kernel functions, you need to release your code with a
GPL compliant license. Please read [LICENSES](./LICENSING.md) for
more information.

<a name="ebpf-loader"></a>
# The loader

Now we can take a pause from the eBPF program and start writing our
loader. To load en eBPF program, the kernel provides the `bpf(2)` system
call. While It is very powerful, this function handles many tasks,
such as loading and unloading the program, creating, 
reading and writing maps, and setting program metadata.
All of this is usually wrapped by an userspace library that
abstract all of the complexity into an easy to use framework.
Libbpf popularized the generation of a `skeleton` program from the
eBPF program, what will help us access the eBPF's data structures
and functions. On a high level, a skeleton is simply a generated 
wrapper of our program.

You can choose any language for the userspace application, as long
as It can make system calls. I chose to use `go` not because I like
It particularly, but because I need to use It for another project so
I want to get better at it. I will use [cilium/ebpf](https://github.com/cilium/ebpf)
to load and interact with the eBPF program.

Now, we will generate the skeleton, load the ebpf program, retrieve
the inode that we want to monitor via the `fstat(2)` system call and
set It in the eBPF's map.

First, we create a Go project with `go mod init hivebpf` and start
writing our main. We will use Go's `log` package for printing information,
as well as `errors` to return errors, `io/ioutil` open a file and
`syscall` to call `fstat(2)`.

```go
package main

import (
	"log"
	"syscall"
	"io/ioutil"
	"errors"

	"github.com/cilium/ebpf/link"
	"github.com/cilium/ebpf/rlimit"
	"github.com/cilium/ebpf"
)
```

To compile our eBPF program we can use `bpf2go` which will compile
everything and generate the skeleton. We can add the following
somewhere in our Go package:
```go
//go:generate go run github.com/cilium/ebpf/cmd/bpf2go -tags linux -cflags "-D __${ARCH}__" bpf ../ebpf/tracer.bpf.c
```

The `ARCH` variable will be set before running `go generate`
```make
hive:
	ARCH=$(shell uname -m) go generate
	go build -o hive
```

Inside our main, we need to remove rlimit cap. This is needed because
kernels newre that 5.11 moved the resource accounting from rlimit to
cGroups so we do not need this limit anymore. This is usually needed,
expecially when running in a contianer (I tried) so It is good to
set It:
```go
// Remove resource limits for kernels <5.11.
if err := rlimit.RemoveMemlock(); err != nil {
	log.Fatalf("Removing memlock: %s", err)
}
```

Now we can load the skeleton:
```go
objs := bpfObjects{}
if err := loadBpfObjects(&objs, nil); err != nil {
	log.Fatalf("Loading eBPF objects: %s", err)
}
defer objs.Close()
```

and attach the kprobe to the kernel symbol:
```go
kprobed_func := "inode_permission"
kp, err := link.Kprobe(kprobed_func, objs.KprobeInodePermission, nil)
if err != nil {
	log.Fatalf("Opening kprobe: %s", err)
}
defer kp.Close()
```

At this point, the eBPF program is loaded inside the kernel and we
can proceed to get the inode and set It in the map:
```go
func GetInode(Target string) (uint64, error) {
	fd, err := syscall.Open(Target, syscall.O_RDONLY, 444)
	if err != nil {
		return 0, err
	}
	defer syscall.Close(fd)

	var stat syscall.Stat_t
	err = syscall.Fstat(fd, &stat)
	if err != nil {
		return 0, err
	}

	return stat.Ino, nil
}

...

Target := "honeypot.txt"
ino, err := GetInode(Target)
if err != nil {
	log.Fatalf("GetInode: %s", err)
}

log.Print("Opened file with inode number ", ino)

var key uint32 = 0
err = objs.TracedInodes.Update(key, ino, ebpf.UpdateAny)
if err != nil {
	log.Fatalf("Updating map: %s", err)
}
```

We can add a loop to keep the application open:
```bash
for {
}
```

<a name="config"></a>
# The config file

As I discussed in the introduction, I want to be able to configure
parts of the application via a config file. I will start with the
simplest case, wich is to change the traced inode by selecting the
path. We provide a config file like this:
```yaml
target: "honeypot.txt"
```

We can parse the config file using `gopkg.in/yaml.v3` wich provides
a simple api: we just need to create a config struct and read
the config file:
```go
import (
	"gopkg.in/yaml.v3"
)

func ParseYaml() (Config, error) {
	data, err := ioutil.ReadFile("config.yaml")
	if err != nil {
		return Config{}, errors.New("Error opening config file")
	}

	var config Config

	err = yaml.Unmarshal(data, &config)
	if err != nil {
		return Config{}, errors.New("Error parsing config file")
	}

	return config, nil
}

// in main
config, err := ParseYaml()
if err != nil {
	log.Fatalf("Error ParseYaml: ", err)
}
```

And we can pass the config target file when getting the inode:
```go
ino, err := GetInode(config.Target)
```

<a name="does-it-work"></a>
# Does It work?

Its now time to test everything. The steps to build are the following:
- compile the eBPF + generate the skeleton
- build the Go loader
- run the application (with sudo)

I created the following Makefile:
```makefile
.PHONY: all hive run
all: hive
hive:
	ARCH=$(shell uname -m) go generate ./cmd
	go build -o hive ./cmd
run:
	sudo ./hive
```

After building everything, our application will register the program
and loop indefinitely. All the output from the eBPF program is
automatically redirected to `/sys/kernel/debug/tracing/trace_pipe`
so we can `cat` this file (requires sudo) and we can try to open
`honeypot.txt` and see if we get some output:
```
<...>-67691   [002] ...21 29217.128425: bpf_trace_printk: {pid = 67691, inode = 431943
```

Nice, It works. We could log more information about the user via
some helper functions, but I will leave this for later.

<a name="kubernetes"></a>
# Kubernetes cluster

Let's now put everything on a cluster and see if this works. There are
many programs to create a cluster locally, I will use `kind` because
It does not emulate hardware so It is really light weight and It is
easy to integrate with local docker images.

First, we create a `Dockerfile` with our application:
```dockerfile
FROM alpine:latest
WORKDIR /app
COPY ./* /app/
RUN chmod +x /app/hive
CMD ["/app/hive"]
```

And we build and test It with:
```bash
sudo docker build -t hive-image .
sudo docker run --privileged hive-image
```

Here, we notice that the container needs to be run in privileged mode
in order to load the program. Moreover, after testing the application,
we notice that the host machine receives the logs when a user inside
the docker container accesses the controlled file. All of this means
that the user of the container needs to have high priviledges on the
host machine in order to load eBPF programs and that containers run on
the same operating system unless they emulate it. Keep this in mind when
you want to use eBPF on a cloud environment.

Second, we need to create a config file where we set one control node
and one worker node:
```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    apiVersion: kubelet.config.k8s.io/v1beta1
    kind: KubeletConfiguration
    evictionHard:
      memory.available: "500Mi"
- role: worker
```

And a config file for the pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hive-app
spec:
  restartPolicy: Never
  containers:
    - name: hive-container
      image: hive-image
      imagePullPolicy: Never
      securityContext:
        privileged: true
```

To create the local cluster, run:
```
sudo kind create cluster --name hive --config kubernetes-config.yaml
```
Note that to delete a cluster you need to run `sudo kind delete cluster`.

Now we can load the previously create docker image:
```bash
sudo /home/santo/apps/kind-linux-amd64 load docker-image hive-image --name hive
```

To run a pod or apply a modification, run:
```bash
sudo kubectl apply -f <my-pod.yaml>
```

We can get a shell inside the pod via:
```bash
sudo kubectl exec -it <my-pod> -- /bin/s
```

To delete the pod:
```bash
sudo kubectl delete pod <my-pod>
```

We can test that everything works now, by accessing the traced inode
and read the logs. Wonderfull.

<a name="further-work"></a>
## Further Work

This project is far from finished or production ready. However, It provides a
base to build upon. Some useful features to implement would be the
following:
- A system to add kprobes from a "controller" node to different pods
  using a Kuberentes operator
  - Update the configuration from a config file during runtime
- Cross-platform and cross-kernel testing, automated using GH actions
- Logging more information in the eBPF program
